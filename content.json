{"pages":[{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"以太坊C++源码解析（三）p2p(1)","text":"整个以太坊p2p代码分为两部分，一部分是基于UDP的节点发现协议，另一部分是基于TCP的传输协议，我们先来看第一部分。这部分主要代码在libp2p\\NodeTable.h文件中。 节点发现协议是采用了类kademlia协议(kademlia-like protocol)，关于kademlia协议可以参看wiki:kademlia我们先来看看NodeTable中整体的流程图：涉及到节点发现协议的四种类型数据包： FindNode Neighbours Ping Pong #doDiscoveryNodeTable类是包含在Host类中的，Host类处理p2p模块的一个接口类，后面会谈到。我们先看NodeTable类的构造函数，里面有这段代码： 12m_socketPointer-&gt;connect();doDiscovery(); 其中第一行的connnect()函数名称有点误导，其实是socket绑定本地端口（默认端口UDP30303），并开始接收外面的数据包。第二行的doDiscovery()函数比较重要，我们来看实现代码： 12345678m_timers.schedule(c_bucketRefresh.count(), [this](boost::system::error_code const&amp; _ec){ // ... NodeID randNodeId; crypto::Nonce::get().ref().copyTo(randNodeId.ref().cropped(0, h256::size)); crypto::Nonce::get().ref().copyTo(randNodeId.ref().cropped(h256::size, h256::size)); doDiscover(randNodeId);}); 无关的代码已省略。可以看到这个函数其实是启动了一个定时器，每隔c_bucketRefresh时间执行一个lambda函数，这个lambda函数产生了一个随机节点，并调用了doDiscover()函数。c_bucketRefresh定义： 1std::chrono::milliseconds const c_bucketRefresh = std::chrono::milliseconds(7200); 得出结论，这个函数就是每隔7200ms刷新一次k桶，也就是产生一个随机节点，并调用doDiscover()。 #doDiscover我们再来看看doDiscover()函数的实现： 1234567891011121314151617181920if (_round == s_maxSteps){ LOG(m_logger) &lt;&lt; \"Terminating discover after \" &lt;&lt; _round &lt;&lt; \" rounds.\"; doDiscovery(); return;}auto nearest = nearestNodeEntries(_node);for (unsigned i = 0; i &lt; nearest.size() &amp;&amp; tried.size() &lt; s_alpha; i++){ auto r = nearest[i]; FindNode p(r-&gt;endpoint, _node); p.sign(m_secret); m_socketPointer-&gt;send(p);}m_timers.schedule(c_reqTimeout.count() * 2, [this, _node, _round, _tried](boost::system::error_code const&amp; _ec){ doDiscover(_node, _round + 1, _tried);}); 注：这里的代码已经经过简化，省略了部分不影响理解流程的代码。这部分代码可以分成三段，第一段和第三段代码都是用来做循环用的，并产生一个定时器，保证每隔c_reqTimeout.count() * 2时间间隔会调用一次doDiscover()，并且保证调用次数不超过s_maxSteps。 这两个常量定义如下： 12std::chrono::milliseconds const c_reqTimeout = std::chrono::milliseconds(300);static unsigned const s_maxSteps = boost::static_log2&lt;s_bits&gt;::value; // 值为8 第二段代码比较重要，这里涉及到了一个重要的函数nearestNodeEntries()，根据字面意思是取最近的节点列表，并向这些节点发送FindNode消息。 #nearestNodeEntriesnearestNodeEntries()函数代码： 123456789101112131415vector&lt;shared_ptr&lt;NodeEntry&gt;&gt; NodeTable::nearestNodeEntries(NodeID _target){ // .. map&lt;unsigned, list&lt;shared_ptr&lt;NodeEntry&gt;&gt;&gt; found; // ... vector&lt;shared_ptr&lt;NodeEntry&gt;&gt; ret; for (auto&amp; nodes: found) for (auto const&amp; n: nodes.second) if (ret.size() &lt; s_bucketSize &amp;&amp; !!n-&gt;endpoint &amp;&amp; n-&gt;endpoint.isAllowed()) ret.push_back(n); return ret;} 同样，为了避免贴大段代码影响读者的信息，我这里做了简化，只贴出重要代码。其实这段代码就是从K桶里把节点取出来，然后按距离从小到大排序，返回序列的前s_bucketSize也就是16个节点。这里巧妙的使用了std::map，并将距离作为key，我们都知道std::map是采用红黑树实现，节点默认会按key从小到大排列，因此把节点放到这个map里就自动排序了，免去了手动排序的过程。这里需要注意的是距离是逻辑上的距离，并无实际意义，只是一种节点的筛选方式，我们可以看下距离的计算方式： 1static int distance(NodeID const&amp; _a, NodeID const&amp; _b) { u256 d = sha3(_a) ^ sha3(_b); unsigned ret; for (ret = 0; d &gt;&gt;= 1; ++ret) {}; return ret; } 可以看到这个距离只是两个节点hash做异或，然后计算二进制最高位为1的位的位数。","link":"/2019/06/21/以太坊C-源码解析（三）p2p-1/"},{"title":"以太坊C++源码解析（三）p2p(2)","text":"K桶的实现这里借用一下别人的图：可以看到这个就是一个二维数组，可以简单定义为：int bucket[256][16]其中第一维是从0-255表示距离，第二维记录16个节点。 在代码中实际的K桶定义是： 1std::array&lt;NodeBucket, s_bins&gt; m_state; 这个定义等同于 1NodeBucket[s_bins] m_state; 其中s_bins值为255，根据上面的图，s_bins应该是256，为什么是255呢？这里其实是把距离为0的那些节点排除掉了，所以为255。再来看NodeBucket的定义： 12345struct NodeBucket{ unsigned distance; std::list&lt;std::weak_ptr&lt;NodeEntry&gt;&gt; nodes;}; 这里的nodes就是实际存放在K桶里的节点了，在这里还看不出来nodes数组的大小，不过在实际添加节点的时候会有大小判断： 123456if (nodes.size() &lt; s_bucketSize){ // ... nodes.push_back(newNode); // ...} s_bucketSize的值正是16. #noteActiveNodenoteActiveNode这个函数在每收到一个UDP包都会被调用，其主要实现代码为： 1234567891011121314151617181920212223242526272829303132333435NodeBucket&amp; s = bucket_UNSAFE(newNode.get());auto&amp; nodes = s.nodes;// check if the node is already in the bucketauto it = std::find(nodes.begin(), nodes.end(), newNode);if (it != nodes.end()){ // if it was in the bucket, move it to the last position nodes.splice(nodes.end(), nodes, it);}else{ if (nodes.size() &lt; s_bucketSize) { // if it was not there, just add it as a most recently seen node // (i.e. to the end of the list) nodes.push_back(newNode); if (m_nodeEventHandler) m_nodeEventHandler-&gt;appendEvent(newNode-&gt;id, NodeEntryAdded); } else { // if bucket is full, start eviction process for the least recently seen node nodeToEvict = nodes.front().lock(); // It could have been replaced in addNode(), then weak_ptr is expired. // If so, just add a new one instead of expired if (!nodeToEvict) { nodes.pop_front(); nodes.push_back(newNode); if (m_nodeEventHandler) m_nodeEventHandler-&gt;appendEvent(newNode-&gt;id, NodeEntryAdded); } }} 这部分是目前我贴出来的最长的代码了，代码虽然长一点，但是可读性非常好，而且有详细的注释。这部分涉及到K桶的操作，收到一个节点后判断是否已经在K桶中了，如果在的话就移动它到nodes的末尾，如果不存在就放到K桶中，这里需要重点注意两行代码： 12if (m_nodeEventHandler) m_nodeEventHandler-&gt;appendEvent(newNode-&gt;id, NodeEntryAdded); 这段代码在节点被放到K桶中时会被调用，这两句非常重要，作用后面再说。 #预设节点这里还有个问题，不知道读者朋友有没有注意到，那就是虽然有节点发现协议来源源不断地发现新节点，但是初始化的时候是只有本节点一个节点的，我从哪里去获得其他节点的IP呢？还记得上一节的nearestNodeEntries()这个函数吗？这个函数是从K桶中取得节点，按距离从小到大排列，然后向这些节点发送FindNode消息的，但是初始情况下K桶是空的，我们要向哪发消息呢？巧妇也难为无米之炊啊，因此这里必然隐藏了一个特殊的绿色通道，用来直接放入初始节点的。这个绿色通道就是NodeTable::addNode()函数，这个函数在多处出现，大部分情况下为正常调用，但是注意到这个函数在Host::addNodeToNodeTable()函数里调用了，我们再来顺藤摸瓜，看看Host::addNodeToNodeTable()在哪里调用，也有多个地方，但是值得注意的是在Host::requirePeer()里的调用。因为这个函数在aleth\\main.cpp里被调用到了，我把调用的地方贴出来： 12for (auto const&amp; i: Host::pocHosts()) web3.requirePeer(i.first, i.second); 这段代码看起来就是在添加一些节点，web3为dev::WebThreeDirect类，而 web3.requirePeer()会调用Host::requirePeer()，至此这些预设节点被添加到NodeTable中，相当于给机器一个初始的力，于是机器开始正常运转起来。有兴趣的读者还可以去Host::pocHosts()看看是哪些初始节点。 #添加节点到传输网络以太坊的P2P模块其实分为两部分，第一部分就是目前说到的UDP节点发现协议，另外一部分是TCP的节点间数据传输协议，这两部分之间是通过什么交互的呢？也就是节点是怎么从第一部分被发现到第二部分和本节点间传输数据呢？其实前面这两句看起来不起眼的代码就是连接两部分之间的桥梁。NodeTable类内置一个事件处理器： 1std::unique_ptr&lt;NodeTableEventHandler&gt; m_nodeEventHandler; NodeTableEventHandler类通过appendEvent()来添加事件，并通过processEvents()来处理每个事件，那么事件是怎么处理的呢？但是processEvent()事件处理函数是一个纯虚函数： 1virtual void processEvent(NodeID const&amp; _n, NodeTableEventType const&amp; _e) = 0; 因此要到它的子类去找，可以看到libp2p\\Host.h文件中定义了子类：HostNodeTableHandler，也实现了processEvent()函数： 1234void HostNodeTableHandler::processEvent(NodeID const&amp; _n, NodeTableEventType const&amp; _e) { m_host.onNodeTableEvent(_n, _e);} 这里又调用了Host::onNodeTableEvent()函数。 1234567891011121314151617181920void Host::onNodeTableEvent(NodeID const&amp; _n, NodeTableEventType const&amp; _e){ if (_e == NodeEntryAdded) { LOG(m_logger) &lt;&lt; \"p2p.host.nodeTable.events.nodeEntryAdded \" &lt;&lt; _n; if (Node n = nodeFromNodeTable(_n)) { shared_ptr&lt;Peer&gt; p; // ... if (peerSlotsAvailable(Egress)) connect(p); } } else if (_e == NodeEntryDropped) { // ... if (m_peers.count(_n) &amp;&amp; m_peers[_n]-&gt;peerType == PeerType::Optional) m_peers.erase(_n); }} 这里同样精简了代码，值得注意的是connect(p);这一句，从字面上看是连接节点，实际代码是通过boost::asio库实现了底层网络传输。现在事情已经比较清楚了，那么NodeTableEventHandler::processEvent()函数是由谁来调用的呢？其实是Host里的一个定时器调用的，有兴趣的读者可以去看Host::run()函数，其实就是Host类与NodeTable类之间的相互调用。","link":"/2019/07/02/以太坊C-源码解析（三）p2p-2/"},{"title":"以太坊C++源码解析（二）大数据类型","text":"我们在C++中常用的表示整形的类型有int, long, unsigned long, int64_t等等，这些类型的长度为32位，64位，一般的情况下就能满足我们的需要了，但是在以太坊里这样的精度类型就显得捉襟见肘了，比如以太坊中常用的货币单位为Wei，而以太币1Ether=10的18次方Wei，这是一个非常大的数，普通的数据类型显然不能用了，我们需要更大的数。 #boost::multiprecision好在C++的boost库里提供了这样的数据类型，它们就存在于boost::multiprecision命名空间中，大家可以在这个网页看到定义：https://www.boost.org/doc/libs/1_65_1/libs/multiprecision/doc/html/boost_multiprecision/tut/ints/cpp_int.html可以看到里面定义了uint128_t，uint256_t，uint512_t甚至uint1024_t。在以太坊ethereum代码中也使用了类似的定义，不过名字不同，采用的u128表示128位无符号数，u256表示256位无符号数，以此类推，具体定义在libdevcore\\Common.h文件中。需要注意的是这种数据类型与字符串类型的转换，因为我们经常需要输出这种数值，转换位字符串会比较方便，这种类型提供了转换为std::string的方法，那就是str()函数，比如： 12u256 value = *******;std::string strValue = value.str(); u128,u256等类型在以太坊代码中使用非常频繁，比如区块头BlockHeader.h中定义了： 123u256 m_gasLimit;u256 m_gasUsed;u256 m_difficulty; m_gasLimit表示gas最大值，m_gasUsed表示所使用的gas，m_difficulty表示挖矿难度。 #FixedHash类FxedHash类是一个模板类，定义于libdevcore\\FixedHash.h中，我们查看这个类定义，可以看到它就只有一个数据成员： 1std::array&lt;byte, N&gt; m_data; 所以这就是一个封装的字节数组类，里面重载了==,!=,&gt;=等符号，还提供了一个成员hex()用于将字节数组转化为字符串数组： 1std::string hex(); 为了方便使用，该文件中还定义了一些别名，比如： 12345using h520 = FixedHash&lt;65&gt;;using h512 = FixedHash&lt;64&gt;;using h256 = FixedHash&lt;32&gt;;using h160 = FixedHash&lt;20&gt;;using h128 = FixedHash&lt;16&gt;; h256表示32个字节大小的字节数组，其他以此类推。FixedHash类在以太坊中也经常用到，通常来表示hash值，我们还是以BlockHeader为例： 12345h256 m_parentHash;h256 m_sha3Uncles;h256 m_stateRoot;h256 m_transactionsRoot;h256 m_receiptsRoot; m_parentHash表示父块的hash值，m_sha3Uncles表示叔块的hash值，m_stateRoot表示state树根hash，m_transactionsRoot表示交易树根hash，m_receiptsRoot表示收据树根hash。","link":"/2019/06/21/以太坊C-源码解析（二）大数据类型/"},{"title":"以太坊C++源码解析（一）Worker类","text":"在ethereum项目中Worker类是许多类的基类，子类们继承Worker类严格来说并不是is-a的关系，而是复用Worker类中的代码，因此可以说Worker类是一份公用代码类，那么是什么代码这么重要呢？ 在Worker类中我们直接来看其成员，可以看到有两个重要的成员变量： 12std::unique_ptr&lt;std::thread&gt; m_work;mutable std::condition_variable m_state_notifier; 看到这里经验丰富的程序员们应该就能想到这个是经典的生产消费者队列的C++11实现方式，还不了解生产消费者队列的同学可以暂停一下，先搜索一下相关知识再继续。然后我们再来看两个重要的成员函数： 12void startWorking();void stopWorking(); 这个其实就是启动和停止m_work这个线程了。在startWorking()函数内部我们能看到线程的初始化过程，线程函数体用的是lambda表达式，其中最重要的一段我列出来： 12345678910try{ startedWorking(); workLoop(); doneWorking();}catch (std::exception const&amp; _e){ cwarn &lt;&lt; \"Exception thrown in Worker thread: \" &lt;&lt; _e.what();} 这个线程最重要的是执行了这三个操作，startedWorking()用于workLoop()前的准备工作,doneWorking()用于workLoop()后的收尾工资，workLoop()内部用一个循环调用了doWork()来做实际的工作： 123456789void Worker::workLoop(){ while (m_state == WorkerState::Started) { if (m_idleWaitMs) this_thread::sleep_for(chrono::milliseconds(m_idleWaitMs)); doWork(); }} 我们再来看这几个函数的定义： 1234virtual void startedWorking() {}virtual void doWork() {}virtual void workLoop();virtual void doneWorking() {} 可以看到除了workLoop()有一个默认的实现外，其他函数体都是空的，子类们通过重新实现这几个函数来实现具体的功能，后面我们会看到这些子类们是怎么做的。","link":"/2019/06/18/以太坊C-源码解析（一）Worker类/"},{"title":"以太坊C++源码解析（三）p2p(3)","text":"我们再来深入了解一下Host类里节点和本节点是怎么交互的，在上一节可以看到节点到了Host类后，会调用Host::connect来连接对方，我们可以看下connect()函数实现代码： 1234567891011121314151617181920212223242526272829303132void Host::connect(std::shared_ptr&lt;Peer&gt; const&amp; _p){ // ... bi::tcp::endpoint ep(_p-&gt;endpoint); cnetdetails &lt;&lt; \"Attempting connection to node \" &lt;&lt; _p-&gt;id &lt;&lt; \"@\" &lt;&lt; ep &lt;&lt; \" from \" &lt;&lt; id(); auto socket = make_shared&lt;RLPXSocket&gt;(m_ioService); socket-&gt;ref().async_connect(ep, [=](boost::system::error_code const&amp; ec) { // ... if (ec) { cnetdetails &lt;&lt; \"Connection refused to node \" &lt;&lt; _p-&gt;id &lt;&lt; \"@\" &lt;&lt; ep &lt;&lt; \" (\" &lt;&lt; ec.message() &lt;&lt; \")\"; // Manually set error (session not present) _p-&gt;m_lastDisconnect = TCPError; } else { cnetdetails &lt;&lt; \"Connecting to \" &lt;&lt; _p-&gt;id &lt;&lt; \"@\" &lt;&lt; ep; auto handshake = make_shared&lt;RLPXHandshake&gt;(this, socket, _p-&gt;id); { Guard l(x_connecting); m_connecting.push_back(handshake); } handshake-&gt;start(); } m_pendingPeerConns.erase(nptr); });} 可以看到先是创建了一个socket，然后用async_connect()异步去连接这个节点，连接成功后生成了一个RLPXHandshake类，并调用了RLPXHandshake::start()来开启握手流程，这里并没有连接成功后就传输数据，因为对方可能并不是一个ethereum节点，或者是运行协议不匹配的节点，握手流程就用来过滤掉不合格的节点，只有通过了握手流程才能进行数据交互。注：在cpp-ethereum项目中底层数据传输用的是boost::asio库，作为准标准库中一员，boost::asio广泛应用在c++跨平台网络开发中，不熟悉的读者建议先去网络上阅读相关文档，后续文档假定读者已经了解了boost::asio库。 #RLPXHandshake类RLPXHandshake::start()函数实际调用了RLPXHandshake::transition()函数，这个函数是RLPXHandshake类的核心，从中可以看到握手的流程。 12345678910111213141516171819202122232425262728293031323334353637383940void RLPXHandshake::transition(boost::system::error_code _ech){ // ... if (m_nextState == New) { m_nextState = AckAuth; if (m_originated) writeAuth(); else readAuth(); } else if (m_nextState == AckAuth) { m_nextState = WriteHello; if (m_originated) readAck(); else writeAck(); } else if (m_nextState == AckAuthEIP8) { m_nextState = WriteHello; if (m_originated) readAck(); else writeAckEIP8(); } else if (m_nextState == WriteHello) { m_nextState = ReadHello; // ... } else if (m_nextState == ReadHello) { // Authenticate and decrypt initial hello frame with initial RLPXFrameCoder // and request m_host to start session. m_nextState = StartSession; // ... }} 精简后的流程还是比较清楚的，初始时候m_nextState值为New，那么正常的握手状态是New -&gt; AckAuth -&gt; WriteHello -&gt; ReadHello -&gt; StartSession。如果这些环节中某一步出错了，那么该节点不会走到最后，否则最后的状态会变成StartSession，那么到了StartSession状态后会发生什么事呢？我们再看看看这部分代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748else if (m_nextState == ReadHello){ // Authenticate and decrypt initial hello frame with initial RLPXFrameCoder // and request m_host to start session. m_nextState = StartSession; // read frame header unsigned const handshakeSize = 32; m_handshakeInBuffer.resize(handshakeSize); ba::async_read(m_socket-&gt;ref(), boost::asio::buffer(m_handshakeInBuffer, handshakeSize), [this, self](boost::system::error_code ec, std::size_t) { if (ec) transition(ec); else { // ... /// rlp of header has protocol-type, sequence-id[, total-packet-size] bytes headerRLP(header.size() - 3 - h128::size); // this is always 32 - 3 - 16 = 13. wtf? bytesConstRef(&amp;header).cropped(3).copyTo(&amp;headerRLP); /// read padded frame and mac m_handshakeInBuffer.resize(frameSize + ((16 - (frameSize % 16)) % 16) + h128::size); ba::async_read(m_socket-&gt;ref(), boost::asio::buffer(m_handshakeInBuffer, m_handshakeInBuffer.size()), [this, self, headerRLP](boost::system::error_code ec, std::size_t) { // ... if (ec) transition(ec); else { // ... try { RLP rlp(frame.cropped(1), RLP::ThrowOnFail | RLP::FailIfTooSmall); m_host-&gt;startPeerSession(m_remote, rlp, move(m_io), m_socket); } catch (std::exception const&amp; _e) { cnetlog &lt;&lt; \"Handshake causing an exception: \" &lt;&lt; _e.what(); m_nextState = Error; transition(); } } }); } });} 当状态从ReadHello向StartSession转变时，连续收了两个包，然后调用了Host::startPeerSession()，节点在RLPXHandshake类转了一圈以后，如果合格的话又回到了Host类中，从此开始新的征程。 #Host类我们之前看到Host类通过requirePeer()函数推动了P2P发现模块的运转，但同时它又是整个P2P传输模块中的发动机，因此要研究ethereum网络部分需要从这里开始。我们在libp2p\\Host.h文件中找到Host类定义，其中有两个成员变量，熟悉boost::asio库的读者一定不陌生： 12ba::io_service m_ioService;bi::tcp::acceptor m_tcp4Acceptor; 其中m_ioService就是Host类的核心了，它负责处理异步任务，当异步任务完成后调用完成句柄。m_tcp4Acceptor是负责接收连接的对象，它内部封装了一个socket对象。我们都知道服务端的socket需要经过创建，绑定IP端口，侦听，Accept这几个阶段，对于m_tcp4Acceptor而言也是这样： 创建 直接在Host类初始化列表中进行创建 绑定IP端口和侦听 这部分是在Network::tcp4Listen()函数中完成的： 123456789101112131415161718192021222324252627for (unsigned i = 0; i &lt; 2; ++i){ bi::tcp::endpoint endpoint(listenIP, requirePort ? _netPrefs.listenPort : (i ? 0 : c_defaultListenPort)); try { /// ... _acceptor.open(endpoint.protocol()); _acceptor.set_option(ba::socket_base::reuse_address(reuse)); _acceptor.bind(endpoint); _acceptor.listen(); return _acceptor.local_endpoint().port(); } catch (...) { // bail if this is first attempt &amp;&amp; port was specificed, or second attempt failed (random port) if (i || requirePort) { // both attempts failed cwarn &lt;&lt; \"Couldn't start accepting connections on host. Failed to accept socket on \" &lt;&lt; listenIP &lt;&lt; \":\" &lt;&lt; _netPrefs.listenPort &lt;&lt; \".\\n\" &lt;&lt; boost::current_exception_diagnostic_information(); _acceptor.close(); return -1; } _acceptor.close(); continue; } } 注意到这里有一个循环，是用来防止端口被占用的。如果第一次端口被占用，则第二次使用0端口，也就是随机端口。在这个函数里，_acceptor依次完成了设置协议，设置端口重用，绑定端口和侦听。 Accept 又回到了Host类，在Host::runAcceptor()函数中，我们能找到以下代码： 12345678910111213141516171819202122232425auto socket = make_shared&lt;RLPXSocket&gt;(m_ioService); m_tcp4Acceptor.async_accept(socket-&gt;ref(), [=](boost::system::error_code ec){ // ... try { // incoming connection; we don't yet know nodeid auto handshake = make_shared&lt;RLPXHandshake&gt;(this, socket); m_connecting.push_back(handshake); handshake-&gt;start(); success = true; } catch (Exception const&amp; _e) { cwarn &lt;&lt; \"ERROR: \" &lt;&lt; diagnostic_information(_e); } catch (std::exception const&amp; _e) { cwarn &lt;&lt; \"ERROR: \" &lt;&lt; _e.what(); } if (!success) socket-&gt;ref().close(); runAcceptor();}); m_tcp4Acceptor通过async_accept()异步接收连接，当一个连接到来的时候发生了什么？我们又看到了熟悉的代码，是的！创建了一个RLPXHandshake类，又开始了握手流程。ethereum对于接收到的连接也是谨慎的，同样需要先进行校验，这里的握手流程与前面connect时的流程稍有不同，区别就在RLPXHandshake::m_originated上，connect时的m_originated值为true，也就是先向对方发送自己的Auth包，而被动接收时m_originated为false，会等待对方发过来Auth包。最后别忘了启动Host::m_ioService，这部分被放在doWork()函数里，还记得doWork()函数吗？因为Host类是从Worker类继承而来，doWork()会在一个循环中被调用。 123456789101112void Host::doWork(){ try { if (m_run) m_ioService.run(); } catch (std::exception const&amp; _e) { // ... }} 但是doWork()不是会被循环调用的吗？难道m_ioService.run()也会重复调用吗？答案是不会，因为m_ioService.run()会阻塞在这里，所以只会执行一次。至此m_tcp4Acceptor能够愉快地接收到TCP连接，并把连接交给RLPXHandshake类去处理了。","link":"/2019/07/02/以太坊C-源码解析（三）p2p-3/"}],"tags":[{"name":"以太坊 C++ 源码","slug":"以太坊-C-源码","link":"/tags/以太坊-C-源码/"}],"categories":[{"name":"以太坊C++源码解析","slug":"以太坊C-源码解析","link":"/categories/以太坊C-源码解析/"}]}